---
title: "Machine Learning"
author: "Arfa Aijazi"
date: "October 2023"
output:
  html_document:
    df_print: paged
---

This script builds a machine learning model to predict temperature-related morbidity

### Setup
Load libraries
```{r echo=TRUE, message=FALSE, warning=FALSE}

# install.packages(c("zoo","xts","quantmod", "ROCR", "abind"))
# install.packages("DMwR_0.4.1.tar", repos=NULL, type="source") # Package DMwR removed from the CRAN repository, so was downloaded from the Archives (https://cran.r-project.org/src/contrib/Archive/DMwR/). This package contains the SMOTE function integrated into caret's cross validation. 

library(tidyverse)
library(caret)
library(DMwR)
library(doParallel)
library(ggpubr)
library(rcartocolor)
library(MLmetrics)
library(pROC)
library(PRROC) # for Precision-Recall curve calculations
library(tictoc)

options(es.use_symbols = TRUE)
```

Import data
```{r}
recs_standardized <- read_csv("../data/recs_standardized.csv") %>%
  mutate(TEMPMA = ifelse(TEMPMA == 1, "TEMPMA", "NONE")) %>%
  mutate(TEMPMA = factor(TEMPMA, levels = c("TEMPMA", "NONE")))

variables <- read_csv("../data/variables.csv")
```
```{r message=FALSE, warning=FALSE, include=FALSE}
# Set plot theme and colors
theme_custom = function() {
  theme_minimal() %+replace%
    theme(legend.position = "top") +
    theme(panel.grid = element_blank()) +
    theme(strip.background = element_blank()) +
    theme(panel.border = element_blank()) +
    theme(panel.background = element_blank()) +
    theme(legend.title = element_blank()) +
    theme(axis.title = element_blank()) +
    theme(text = element_text(size = 7, colour = "#000000"))
}
```

Color palette
```{r}
## custom colors
my_pal <- carto_pal(n = 5, name = "Vivid")
```

```{r}
# Create 100 random seeds for training/test split
# Split data into training and test
# Set training control variable
# Iterate through input features group (Baseline vs. All)
# Iterate through sampling methods ()
# Iterate through machine learning model types
```

### Machine learning model building
```{r}
bootstrap = sample(1:1000, 1)
input_features = c("Baseline") #c("Baseline", "All")
imbalance = c("up") #c("none", "weighted", "up", "smote", "rose")

results_compiled <- data.frame(bootstrap = integer(), input_features = character(), imbalance = character(), mlMethod = character, time.start = double(), time.end = double(), Precision = double(), Recall = double(), F1 = double(), BalancedAccuracy = double())

for (i in bootstrap) {
  set.seed(i)
  trainIndex <- createDataPartition(recs_standardized$TEMPMA, p = 0.8, list = FALSE, times = 1)
  
  trainData <- recs_standardized[trainIndex,]
  testData <- recs_standardized[-trainIndex,]
  
  set.seed(i)
  ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 5,
                     summaryFunction = prSummary,
                     classProbs = TRUE,
                     index = createFolds(trainData$TEMPMA, 5),
                     allowParallel = TRUE)
  
  model_weights <- ifelse(trainData$TEMPMA == "TEMPMA",
                        (1/table(trainData$TEMPMA)[1]) * 0.5,
                        (1/table(trainData$TEMPMA)[2]) * 0.5)
  
  for (j in input_features) {
    if (j == "Baseline") {
      vars <- variables %>%
        filter(BASELINE == 1)
      
       training <- trainData %>%
         select(starts_with(vars$VARIABLE))
       training$TEMPMA <- trainData$TEMPMA
    }
    
    else {
      training <- trainData
    }
    
    mlMethod = list(c(method = "glm", NULL), # algorithm has no tuning parameters
                c(method = "earth", expand.grid(degree = 1, 
                                                nprune = round(seq(1, 100, length = 100)))),
                c(method = "pda", expand.grid(lambda = seq(0, 0.1, length = 100))),
                c(method = "multinom", expand.grid(decay = seq(0, 0.1, length = 100))),
                c(method = "treebag", NULL), # algorithm has no tuning parameters
                c(method = "gbm", expand.grid(interaction.depth = 1, 
                                              n.trees = seq(50, 500, length = 10), 
                                              shrinkage = 5*10^seq(-2, -3, length = 10),
                                              n.minobsinnode = 10)),
                c(method = "ranger", expand.grid(mtry = round(seq(1, ncol(training)-1, length = 10)),
                                                 splitrule = c("gini", "extratrees"),
                                                 min.node.size = seq(1,5, length = 5))),
                c(method = "nnet", expand.grid(size = round(seq(1, ncol(training)-1, length = 10)), 
                         decay = 10^seq(-7, -1, length = 10))))
    
    for (k in imbalance) {
      if (k == "none" | k == "weighted") {
        ctrl$sampling = NULL
      } else {
        ctrl$sampling = k
      }
      
      for (m in mlMethod) {
        start <- Sys.time()
        cl <- makePSOCKcluster(detectCores()-1) # Reserve 1 core for operating system
        registerDoParallel(cl) # Start parallel processing
        set.seed(123)
        if (m[[1]] == "glm" | m[[1]] == "treebag") { # these ML algorithms do not have tuning parameters
          train_fit <- train(TEMPMA ~ .,
                          data = training,
                          method = m[[1]],
                          metric = "AUC",
                          trControl = ctrl)
        }
        else {
          train_fit <- train(TEMPMA ~ .,
                          data = training,
                          method = m[[1]],
                          metric = "AUC",
                          trControl = ctrl,
                          tuneGrid = data.frame(m[-1]))
        }
        stopCluster(cl) # End parallel processing
        end <- Sys.time()
        
        predictions <- predict(train_fit, testData)
        confusionMatrix_output <- confusionMatrix(data = predictions, reference = testData$TEMPMA)
    
        results <- data.frame(bootstrap = i, 
                              input_features = j, 
                              imbalance = k, 
                              mLMethod = m[[1]], 
                              time.start = start, 
                              time.end = end, 
                              Precision = confusionMatrix_output$byClass[["Precision"]], 
                              Recall = confusionMatrix_output$byClass[["Recall"]], 
                              F1 = confusionMatrix_output$byClass[["F1"]],
                              BalancedAccuracy = confusionMatrix_output$byClass[["Balanced Accuracy"]])
        results_compiled <- rbind(results_compiled, results)
      }
    }
  }
}


```
### Export
```{r}
write_csv(results_compiled, "../data/results_compiled.csv")
```


