---
title: "Machine Learning"
author: "Arfa Aijazi"
date: "October 2023"
output:
  html_document:
    df_print: paged
---

This script builds a machine learning model to predict temperature-related morbidity

### Setup
Load libraries
```{r echo=TRUE, message=FALSE, warning=FALSE}

# install.packages(c("zoo","xts","quantmod", "ROCR", "abind"))
# install.packages("DMwR_0.4.1.tar", repos=NULL, type="source") # Package DMwR removed from the CRAN repository, so was downloaded from the Archives (https://cran.r-project.org/src/contrib/Archive/DMwR/). This package contains the SMOTE function integrated into caret's cross validation. 

library(tidyverse)
library(caret)
library(caretEnsemble)
library(DMwR)
library(doParallel)


options(es.use_symbols = TRUE)
```

Import data
```{r}
recs_range <- read_csv("../data/recs_range.csv") %>%
  mutate(TEMPMA = ifelse(TEMPMA == 1, "TEMPMA", "NONE")) %>%
  mutate(TEMPMA = factor(TEMPMA))
```

### Machine learning model building
Partition data
```{r}
set.seed(3456)
trainIndex <- createDataPartition(recs_range$TEMPMA, p = 0.8, list = FALSE, times = 1)

trainData <- recs_range[trainIndex,]
testData <- recs_range[-trainIndex,]

X <- trainData %>% select(-TEMPMA)
Y <- trainData$TEMPMA
```

Baseline model
```{r}
models <- read_csv("../data/models.csv")
baseline <- models %>%
  filter(BASELINE == 1)

trainData_baseline <- trainData %>%
  select(starts_with(baseline$VARIABLE))

trainData_baseline$TEMPMA <- Y
```

Create a custom SMOTE function to use within resampling. The perc.over parameter was set to generate 9x more samples of the minority class (households with temperature-related morbidity). The percent.under parameter was set to retain all samples of the majority class (households without temperature-related morbidity). Overall this results in temperature-related morbidity accounting for around 10% of the training data vs 1% in the real data. 
```{r}
smote2 <- list(name = "SMOTE with custom parameters",
                func = function (x, y) {
                  library(DMwR)
                  dat <- if (is.data.frame(x)) x else as.data.frame(x)
                  dat$.y <- as.factor(y)
                  dat <- SMOTE(.y ~ ., 
                               data = dat, 
                               perc.over = 900,
                               perc.under = 1053,
                               k = 13)
                  list(x = dat[, !grepl(".y", colnames(dat), fixed = TRUE)], 
                       y = dat$.y)
                  },
                first = TRUE)
```

Hyperparameter tuning
```{r}
# Cross validate with adaptive resampling. This method resamples the hyperparamter combinations with values near combinations that performed well. It will divide the training data into k folds and repeat the process n times. The model with test t random hyperparamter values

k = 10
n = 10

set.seed(1234) 
adaptControl <- trainControl(method = "adaptive_cv",
                     number = k, repeats = n, 
                     adaptive = list(min = 5, alpha = 0.01, method = "gls", complete = TRUE),
                     classProbs = TRUE,
                     index = createFolds(trainData$TEMPMA, k), # ensures all models have the same folds
                     summaryFunction = twoClassSummary,
                     allowParallel = TRUE
                     )
```

```{r}
# Number of hyperparameters to test
t = 10

# Set up tuning grids
ridge_grid <- expand.grid(alpha = 0, lambda = c(0, 10^seq(5, -5, length = t-1)))
lasso_grid <- expand.grid(alpha = 1, lambda = c(0, 10^seq(5, -5, length = t-1)))
svmLinear_grid <- expand.grid(C = 10^seq(-5, 5, length = t))
svmRadial_grid <- expand.grid(C = seq(0, 5, length = sqrt(t)), sigma = 2^seq(-3, 5, length = sqrt(t))) # rule of thumb that optimal k in knn is sqrt(n)
knn_grid <- expand.grid(k = round(seq(1, sqrt(nrow(trainData)), length = t))) %>% 
  mutate(k = ifelse((k %% 2) == 0, k+1, k)) # ensure knn has an odd number for k to prevent ties in classifying two classes
rf_grid <- expand.grid(mtry = seq(1, 25, length = t))
nn_grid <- expand.grid(size = seq(1, 20, length = sqrt(t)), decay = 10^seq(-7, -1, length = sqrt(t)))
```

Parallel processing
```{r}
cl <- makePSOCKcluster(detectCores()-1)
registerDoParallel(cl)
```

```{r}
model_list_all_none <- caretList(TEMPMA ~ ., 
                        data = trainData, 
                        trControl = adaptControl,
                        tuneList=list(
                          ridge = caretModelSpec(method = "glmnet", tuneGrid = ridge_grid),
                          lasso = caretModelSpec(method = "glmnet", tuneGrid = lasso_grid),
                          svmLinear = caretModelSpec(method = "svmLinear", tuneGrid = svmLinear_grid),
                          svmRadial = caretModelSpec(method = "svmRadial", tuneGrid = svmRadial_grid),
                          knn = caretModelSpec(method = "knn", tuneGrid = knn_grid),
                          rf = caretModelSpec(method = "rf", tuneGrid = rf_grid),
                          nn = caretModelSpec(method = "nnet", tuneGrid = nn_grid)
                        ))
```

End parallel processing
```{r}
stopCluster(cl)
```

