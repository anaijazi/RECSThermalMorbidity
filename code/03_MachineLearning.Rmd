---
title: "Machine Learning"
author: "Arfa Aijazi"
date: "October 2023"
output:
  html_document:
    df_print: paged
---

This script builds a machine learning model to predict temperature-related morbidity

### Setup
Load libraries
```{r echo=TRUE, message=FALSE, warning=FALSE}

# install.packages(c("zoo","xts","quantmod", "ROCR", "abind"))
# install.packages("DMwR_0.4.1.tar", repos=NULL, type="source") # Package DMwR removed from the CRAN repository, so was downloaded from the Archives (https://cran.r-project.org/src/contrib/Archive/DMwR/). This package contains the SMOTE function integrated into caret's cross validation. 

library(tidyverse)
library(caret)
library(caretEnsemble)
library(DMwR)
library(doParallel)
library(pracma)


options(es.use_symbols = TRUE)
```

Import data
```{r}
recs_range <- read_csv("../data/recs_range.csv")  %>%
  mutate(TEMPMA = ifelse(TEMPMA == 1, "TEMPMA", "NONE")) %>%
  mutate(TEMPMA = factor(TEMPMA, levels = c("TEMPMA", "NONE")))
```

### Machine learning model building
Partition data
```{r}
set.seed(3456)
trainIndex <- createDataPartition(recs_range$TEMPMA, p = 0.8, list = FALSE, times = 1)

trainData <- recs_range[trainIndex,]
testData <- recs_range[-trainIndex,]

X <- trainData %>% select(-TEMPMA)
Y <- trainData$TEMPMA
```

Hyperparameter tuning
```{r}
# Cross validate with adaptive resampling. This method resamples the hyperparamter combinations with values near combinations that performed well. It will divide the training data into k folds and repeat the process n times. The model with test t random hyperparamter values

k = 10
n = 3

set.seed(1234) 
adaptControl <- trainControl(method = "adaptive_cv",
                     number = k, repeats = n, 
                     adaptive = list(min = 3, alpha = 0.05, method = "gls", complete = FALSE),
                     classProbs = TRUE,
                     index = createFolds(trainData$TEMPMA, k), # ensures all models have the same folds
                     summaryFunction = twoClassSummary,
                     allowParallel = TRUE
                     )
```

Custom train function
```{r}
train_recs <- function(training_data, ctrl, t) {
  # Set up hyperparameter grid with t elements
  ridge_grid <- expand.grid(alpha = 0, lambda = c(0, 10^seq(5, -5, length = t-1)))
  lasso_grid <- expand.grid(alpha = 1, lambda = c(0, 10^seq(5, -5, length = t-1)))
  svmLinear_grid <- expand.grid(C = 10^seq(-5, 5, length = t))
  svmRadial_grid <- expand.grid(C = seq(0.1, 5, length = sqrt(t)), sigma = 2^seq(-3, 5, length = sqrt(t)))
  knn_grid <- expand.grid(k = round(seq(1, sqrt(nrow(training_data)), length = t))) %>% # rule of thumb that optimal k in knn is sqrt(n)
    mutate(k = ifelse((k %% 2) == 0, k+1, k)) # ensure knn has an odd number for k to prevent ties in classifying two classes
  rf_grid <- expand.grid(mtry = round(seq(1, ncol(training_data)-1, length = t)))
  nn_grid <- expand.grid(size = round(seq(1, ncol(training_data)-1, length = sqrt(t))), decay = 10^seq(-7, -1, length = sqrt(t)))
  
  # Models list
  mlModels <- list(ridge = caretModelSpec(method = "glmnet", tuneGrid = ridge_grid),
                   lasso = caretModelSpec(method = "glmnet", tuneGrid = lasso_grid),
                   svmLinear = caretModelSpec(method = "svmLinear", tuneGrid = svmLinear_grid),
                   svmRadial = caretModelSpec(method = "svmRadial", tuneGrid = svmRadial_grid),
                   knn = caretModelSpec(method = "knn", tuneGrid = knn_grid),
                   rf = caretModelSpec(method = "rf", tuneGrid = rf_grid),
                   nn = caretModelSpec(method = "nnet", tuneGrid = nn_grid)
                   )
  
  # Train model
  return(caretList(TEMPMA ~ .,
                   data = training_data,
                   trControl = ctrl,
                   tuneList = mlModels))
}
```

```{r}
subsamples <- c("none", "up", "smote")
models <- c("Baseline", "Buildings", "All")
t = 5

variables <- read_csv("../data/variables.csv")
results_resamples <- data.frame()

for (i in subsamples) {
  for (j in models) {
    # Select columns for training data
    if(j == "Baseline") {
      vars <- variables %>%
        filter(BASELINE == 1)
      training <- trainData %>%
        select(starts_with(vars$VARIABLE))
      }
    else if(j == "Buildings") {
      vars <- variables %>%
        filter(BUILDINGS == 1)
      training <- trainData %>%
        select(starts_with(vars$VARIABLE))
      }
    else {
      training <- trainData
      }
    training$TEMPMA <- Y
    # Set sampling
    if (i == "none") {
    adaptControl$sampling <- NULL # Due to error that sampling cannot equal "none" https://github.com/topepo/caret/issues/1001
  }
    else {
      adaptControl$sampling <- i
    }
    cl <- makePSOCKcluster(detectCores()-1) # Reserve 1 core for operating system
    registerDoParallel(cl) # Start parallel processing
    
    set.seed(1234)
    train_output <- train_recs(training, adaptControl, t)
    
    stopCluster(cl) # End parallel processing
    
    results_ROC <- as.data.frame(resamples(train_output), metric = "ROC") %>%
      mutate(Model = j) %>%
      mutate(Sampling = i) %>%
      mutate(Metric = "ROC")
    
    results_Sens <- as.data.frame(resamples(train_output), metric = "Sens") %>%
      mutate(Model = j) %>%
      mutate(Sampling = i) %>%
      mutate(Metric = "Sens")
    
    results_Spec <- as.data.frame(resamples(train_output), metric = "Spec") %>%
      mutate(Model = j) %>%
      mutate(Sampling = i) %>%
      mutate(Metric = "Spec")
    
    results_resamples <- rbind(results_resamples, results_ROC, results_Sens, results_Spec)
 
  }
}

  
```
```{r}
results_long <- results_resamples %>%
  select(-Resample) %>%
  pivot_longer(cols = ridge:nn, names_to = "MLMethod", values_to = "Value")

ggplot(results_long, aes(x= Model, y = Value, fill = Sampling)) +
  geom_boxplot() +
  facet_wrap(vars(Metric)) +
  theme_minimal() +
  coord_flip()
```

